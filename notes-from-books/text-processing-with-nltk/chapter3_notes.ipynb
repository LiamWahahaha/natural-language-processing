{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating custom corpora\n",
    "\n",
    "- Setting up a custom corpus\n",
    "- Creating a wordlist corpus\n",
    "- Creating a part-of-speech tagged word corpus\n",
    "- Creating a chunked phrase corpus\n",
    "- Creating a categorized text corpus\n",
    "- Creating a categorized chunk corpus reader\n",
    "- Lazy corpus loading\n",
    "- Creating a custom corpus view\n",
    "- Creating a MongoDB-backed corpus reader\n",
    "- Corpus editing with file locking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inheritance diagrams\n",
    "\n",
    "- `CorpusReader`\n",
    "  - fileids  \n",
    "- `PlaintextCorpusReader(CorpusReader)`\n",
    "  - words, sents, paras  \n",
    "- `WordListCorpusReader(CorpusReader)`\n",
    " - words  \n",
    "- `TaggedCorpusReader(CorpusReader)`\n",
    " - words, sents, paras, tagged_words, tagged_sents, tagged_paras  \n",
    "- `ChunkedCorpusReader(CorpusReader)`\n",
    " - words, sents, paras, tagged_words, tagged_sents, tagged_paras, chunked_words, chunked_sents, chunked_paras  \n",
    "- `ConllCorpusReader(CorpusReader)`\n",
    " - words, sents, paras, tagged_words, tagged_sents, iob_words, iob_sents\n",
    "- `ConllChunkCorpusReader(ConllCorpusReader)`\n",
    " - words, sents, paras, tagged_words, tagged_sents, iob_words, iob_sents, chunked_words, chunked_sents  \n",
    "- `CategorizedCorpusReader`\n",
    " - categories, fileid  \n",
    "- `CategorizedPlaintextCorpusReader(PlaintextCorpusReader, CategorizedCorpusReader)`\n",
    "- Not provided by NLTK\n",
    " - `CategorizedChunkedCorpusReader(ChunkedCorpusReader, CategorizedCorpusReader)`\n",
    " - `CategorizexConllChunkCorpusReader(ConllCorpusReader, ConllChunkCorpusReader, CategorizedCorpusReader)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up a custom corpus\n",
    "\n",
    "- A `corpus` is a collection of text documents, and `corpora` is the plural of corpus.\n",
    "- A `custom corpus` is a bunch of text files in a directory, often alongside many other directories of text files.\n",
    "\n",
    "### How to do it?\n",
    "\n",
    "- NLTK defines a list of data directories, or paths, in nltk.data.path. Custom corpora must be within one of these paths so it can be found by NLTK.\n",
    "- In order to avoid conflict with the official data package, we'll create a custom nltk_data directory in our home directory.\n",
    "- We can create a simple wordlist file and make sure it loads.\n",
    " - Create a subdirectory in corpora to hold our custom corpus\n",
    " - Create a wordlist file and put this file into the subdirectory\n",
    "- nltk.data.load can also load pickle files and .yaml files\n",
    "- For most corpora access, we won't actually need to use nltk.data.load, since that will be handled by the `CorpusReader` classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, os.path\n",
    "path = os.path.expanduser('~/nltk_data')\n",
    "if not os.path.exists(path):\n",
    "    print('path does not exist')\n",
    "    os.mkdir(path)\n",
    "os.path.exists(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk.data\n",
    "path in nltk.data.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'nltk\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk.data\n",
    "nltk.data.load('corpora/cookbook/mywords.txt', format='raw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading a YAML file\n",
    "import nltk.data\n",
    "nltk.data.load('corpora/cookbook/synonyms.yaml')\n",
    "=> {'bday': 'birthday'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a wordlist corpus\n",
    "\n",
    "The `WordListCorpusReader` class is one of the simplest `CorpusReader` classes.\n",
    "- Provide access to a file containing a list of words, one word per line\n",
    "- When we call the words() function, it calls nltk.tokenize.line_tokenize() on the raw file data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reader words:  ['nltk', 'nltk', 'corpus', 'corpora', 'wordnet']\n",
      "reader file id:  ['mywords.txt', 'wordlist.txt']\n",
      "step by step:  ['nltk', 'nltk', 'corpus', 'corpora', 'wordnet']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus.reader import WordListCorpusReader\n",
    "\n",
    "wordlist_dir = nltk.data.find('corpora/cookbook')\n",
    "reader = WordListCorpusReader(wordlist_dir,'.*\\.txt')\n",
    "print('reader words: ', reader.words())\n",
    "print('reader file id: ', reader.fileids())\n",
    "\n",
    "# equivalent as follows:\n",
    "reader.raw()\n",
    "from nltk.tokenize import line_tokenize\n",
    "print('step by step: ', line_tokenize(reader.raw()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/liam/nltk_data/corpora/cookbook\n"
     ]
    }
   ],
   "source": [
    "wordlist_dir = nltk.data.find('corpora/cookbook')\n",
    "print(wordlist_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Names wordlist corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "names wordlist corpus: female.txt. This corpus contains 5001 names.\n",
      "names wordlist corpus:   male.txt. This corpus contains 2943 names.\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import names\n",
    "for fileid in names.fileids():\n",
    "    print('names wordlist corpus: {:>10}. This corpus contains {:4} names.'.format(fileid, len(names.words(fileid))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English words corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words wordlist corpus:         en. This corpus contains 235886 words.\n",
      "words wordlist corpus:   en-basic. This corpus contains    850 words.\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import words\n",
    "for fileid in words.fileids():\n",
    "    print('words wordlist corpus: {:>10}. This corpus contains {:6} words.'.format(fileid, len(words.words(fileid))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a part-of-speech tagged word corpus\n",
    "\n",
    "- Part-of-speech tagging is the process of identifying the part-of-speech tag for a word.\n",
    " - Most of the time, a tagger must first be trained on a training corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The simplest format for a tagged corpus is of the form word/tag.\n",
    "- An excerpt from the brown corpus:\n",
    " - The/at-tl expense/nn and/cc time/nn involved/vbn are/ber astronomical/jj ./.  \n",
    "- Different corpora can use different tags to mean the same thing.\n",
    "\n",
    "### What is the TaggedCorpusReader class?\n",
    "- TaggedCorpusReader provides several methods for extracting text from a corpus.\n",
    " - words()\n",
    " - sents()\n",
    " - paras()\n",
    " - tagged_words()\n",
    " - tagged_sents()\n",
    " - tagged_paras()\n",
    "\n",
    "### Tonkenizer can be customized.\n",
    "#### Customizing the word tokenizer\n",
    "- The default word tokenizer is an instance of nltk.tokenize.WhitespaceTokeniser\n",
    "- We can pass different tokenizer to word_tokenizer\n",
    "\n",
    "#### Customizing the sentence tokenizer\n",
    "- The default sentence tokenizer is an instance of nltk.tokenize.RegexpTokenize with '\\n'\n",
    " - Assuming that each sentence is on a line all by itself, and individual sentences do not have line breaks.\n",
    "- We can pass different tokenizer to sent_tokenizer\n",
    "\n",
    "#### Customizing the paragraph block reader\n",
    "- The Paragraphs are assumed to be split by blank lines.\n",
    " - This is done with the para_block_reader function in nltk.corpus.reader.util\n",
    "- There are a number of other block reader functions in nltk.corpus.reader.util, whose purpose is to read blocks of text from a stream\n",
    "\n",
    "#### Customizing the tag separator\n",
    "- The default is sep = '/'\n",
    "- If we want to split words and tags with '|', we should pass in sep = '|'\n",
    "\n",
    "#### Converting tags to a universal tagset\n",
    "- NLTK provides a method for converting known tagsets to a universal tagset.\n",
    "- A tagset is just a list of part-of-speech tags used by one or more corpora.\n",
    "- To map corpus tags to the universal tagset, the corpus reader must be initialized with a known tagset name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words:  ['The', 'expense', 'and', 'time', 'involved', 'are', ...]\n",
      "sents:  [['The', 'expense', 'and', 'time', 'involved', 'are'], ['astronomical', '.']]\n",
      "paras:  [[['The', 'expense', 'and', 'time', 'involved', 'are'], ['astronomical', '.']]]\n",
      "tagged_words:  [('The', 'AT-TL'), ('expense', 'NN'), ('and', 'CC'), ...]\n",
      "tagged_sents:  [[('The', 'AT-TL'), ('expense', 'NN'), ('and', 'CC'), ('time', 'NN'), ('involved', 'VBN'), ('are', 'BER')], [('astronomical', 'JJ'), ('.', '.')]]\n",
      "tagged_paras:  [[[('The', 'AT-TL'), ('expense', 'NN'), ('and', 'CC'), ('time', 'NN'), ('involved', 'VBN'), ('are', 'BER')], [('astronomical', 'JJ'), ('.', '.')]]]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus.reader import TaggedCorpusReader\n",
    "\n",
    "reader = TaggedCorpusReader(wordlist_dir, r'.*\\.pos')\n",
    "print('words: ', reader.words())\n",
    "print('sents: ', reader.sents())\n",
    "print('paras: ', reader.paras())\n",
    "print('tagged_words: ', reader.tagged_words())\n",
    "print('tagged_sents: ', reader.tagged_sents())\n",
    "print('tagged_paras: ', reader.tagged_paras())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples for converting tags to a universal tagset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DET'), ('expense', 'NOUN'), ('and', 'CONJ'), ...]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader = TaggedCorpusReader(wordlist_dir, r'.*\\.pos', tagset='en-brown')\n",
    "reader.tagged_words(tagset='universal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ...]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import treebank\n",
    "treebank.tagged_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Pierre', 'NOUN'), ('Vinken', 'NOUN'), (',', '.'), ...]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treebank.tagged_words(tagset='universal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Pierre', 'UNK'), ('Vinken', 'UNK'), (',', 'UNK'), ...]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treebank.tagged_words(tagset='brown')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a chunked phrase corpus\n",
    "\n",
    "A chunk is a short phrase within a sentence.\n",
    "\n",
    "### What is the ChunkedCorpusReader class?\n",
    "- ChunkedCorpusReader provides several methods for extracting text from a corpus.\n",
    " - words()\n",
    " - sents()\n",
    " - paras()\n",
    " - tagged_words()\n",
    " - tagged_sents()\n",
    " - tagged_paras()\n",
    " - chunked_words()\n",
    " - chunked_sents()\n",
    " - chunked_paras()\n",
    "- We can draw a tree by calling the draw() method.\n",
    "\n",
    "### What is ConllChunkCorpusReader class?\n",
    "- ConllChunkCorpusReader provides several methods as follows:\n",
    " - words()\n",
    " - sents()\n",
    " - tagged_words()\n",
    " - tagged_sents()\n",
    " - chunked_words()\n",
    " - chunked_sents()\n",
    " - iob_words()\n",
    " - iob_sents()\n",
    "- An alternative format for denoting chunks is called IOB tags.\n",
    " - IOB tags are similar to part-of-speech tags, but provide a way to denote the inside, outside, and beginning of a chunk.\n",
    " - To read a corpus using the IOB format, we must use the ConllChunkCorpusReader class.\n",
    "  - Each sentence is separated by a blank line, but there is no separation for paragraphs.\n",
    "   - This means that the para_* methods are not available\n",
    "   \n",
    "### Tree leaves\n",
    "- When it comes to chunk trees, the leaves of a tree are the tagged tokens.\n",
    "\n",
    "### Treebank chunk corpus\n",
    "- The nltk.corpus.treebank_chunk corpus uses ChunkedCorpusReader to provide part-of-speech tagged words and noun phrase chunks of Wall Street Journal headlines.\n",
    "\n",
    "### CoNLL2000 corpus\n",
    "- CoNLL stands for the Conference on Computational Natural Language Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tree('NP', [('Earlier', 'JJR'), ('staff-reduction', 'NN'), ('moves', 'NNS')]), ('have', 'VBP'), ...]\n",
      "[Tree('S', [Tree('NP', [('Earlier', 'JJR'), ('staff-reduction', 'NN'), ('moves', 'NNS')]), ('have', 'VBP'), ('trimmed', 'VBN'), ('about', ''), ('IN', None), Tree('NP', [('300', 'CD'), ('jobs', 'NNS')]), (',', ','), Tree('NP', [('the', 'DT'), ('spokesman', 'NN')]), ('said', 'VBD'), ('.', '.')])]\n",
      "[[Tree('S', [Tree('NP', [('Earlier', 'JJR'), ('staff-reduction', 'NN'), ('moves', 'NNS')]), ('have', 'VBP'), ('trimmed', 'VBN'), ('about', ''), ('IN', None), Tree('NP', [('300', 'CD'), ('jobs', 'NNS')]), (',', ','), Tree('NP', [('the', 'DT'), ('spokesman', 'NN')]), ('said', 'VBD'), ('.', '.')])]]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus.reader import ChunkedCorpusReader\n",
    "\n",
    "reader = ChunkedCorpusReader(wordlist_dir, r'.*\\.chunk')\n",
    "print(reader.chunked_words())\n",
    "print(reader.chunked_sents())\n",
    "print(reader.chunked_paras())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Earlier', 'JJR'), ('staff-reduction', 'NN'), ('moves', 'NNS')]\n",
      "[('Earlier', 'JJR'), ('staff-reduction', 'NN'), ('moves', 'NNS'), ('have', 'VBP'), ('trimmed', 'VBN'), ('about', ''), ('IN', None), ('300', 'CD'), ('jobs', 'NNS'), (',', ','), ('the', 'DT'), ('spokesman', 'NN'), ('said', 'VBD'), ('.', '.')]\n",
      "[('Earlier', 'JJR'), ('staff-reduction', 'NN'), ('moves', 'NNS'), ('have', 'VBP'), ('trimmed', 'VBN'), ('about', ''), ('IN', None), ('300', 'CD'), ('jobs', 'NNS'), (',', ','), ('the', 'DT'), ('spokesman', 'NN'), ('said', 'VBD'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(reader.chunked_words()[0].leaves())\n",
    "print(reader.chunked_sents()[0].leaves())\n",
    "print(reader.chunked_paras()[0][0].leaves())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader.chunked_sents()[0].draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tree('NP', [('Mr.', 'NNP'), ('Meador', 'NNP')]), Tree('VP', [('had', 'VBD'), ('been', 'VBN')]), ...]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus.reader import ConllChunkCorpusReader\n",
    "\n",
    "conllreader = ConllChunkCorpusReader(wordlist_dir, r'.*\\.iob', ('NP', 'VP', 'PP'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tree('NP', [('Mr.', 'NNP'), ('Meador', 'NNP')]), Tree('VP', [('had', 'VBD'), ('been', 'VBN')]), ...]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conllreader.chunked_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tree('S', [Tree('NP', [('Mr.', 'NNP'), ('Meador', 'NNP')]), Tree('VP', [('had', 'VBD'), ('been', 'VBN')]), Tree('NP', [('executive', 'JJ'), ('vice', 'NN'), ('president', 'NN')]), Tree('PP', [('of', 'IN')]), Tree('NP', [('Balcor', 'NNP')]), ('.', '.')])]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conllreader.chunked_sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Mr.', 'NNP', 'B-NP')\n",
      "('Meador', 'NNP', 'I-NP')\n",
      "('had', 'VBD', 'B-VP')\n",
      "('been', 'VBN', 'I-VP')\n",
      "('executive', 'JJ', 'B-NP')\n",
      "('vice', 'NN', 'I-NP')\n",
      "('president', 'NN', 'I-NP')\n",
      "('of', 'IN', 'B-PP')\n",
      "('Balcor', 'NNP', 'B-NP')\n",
      "('.', '.', 'O')\n"
     ]
    }
   ],
   "source": [
    "for elem in conllreader.iob_words():\n",
    "    print(elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Mr.', 'NNP', 'B-NP'), ('Meador', 'NNP', 'I-NP'), ('had', 'VBD', 'B-VP'), ('been', 'VBN', 'I-VP'), ('executive', 'JJ', 'B-NP'), ('vice', 'NN', 'I-NP'), ('president', 'NN', 'I-NP'), ('of', 'IN', 'B-PP'), ('Balcor', 'NNP', 'B-NP'), ('.', '.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "for elem in conllreader.iob_sents():\n",
    "    print(elem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a categorized text corpus\n",
    "\n",
    "If we have a large corpus of text, we might want to categorize it into separate sections. `This could be helpful for text classification.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adventure',\n",
       " 'belles_lettres',\n",
       " 'editorial',\n",
       " 'fiction',\n",
       " 'government',\n",
       " 'hobbies',\n",
       " 'humor',\n",
       " 'learned',\n",
       " 'lore',\n",
       " 'mystery',\n",
       " 'news',\n",
       " 'religion',\n",
       " 'reviews',\n",
       " 'romance',\n",
       " 'science_fiction']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "brown.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category: neg  files: ['movie_neg.txt']\n",
      "category: pos  files: ['movie_pos.txt']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus.reader import CategorizedPlaintextCorpusReader\n",
    "\n",
    "# option 1\n",
    "reader = CategorizedPlaintextCorpusReader('corpora', r'movie_.*\\.txt', cat_pattern = r'movie_(\\w+)\\.txt')\n",
    "for cat in reader.categories():\n",
    "    print('category: {:4} files: {}'.format(cat, reader.fileids(cat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neg', 'pos']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# option 2\n",
    "reader = CategorizedPlaintextCorpusReader('corpora', r'movie_.*\\.txt',\n",
    "                                          cat_map = {'movie_pos.txt':['pos'], 'movie_neg.txt':['neg']})\n",
    "reader.categories()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is CategorizedPlaintextCorpusReader class and how it works?\n",
    "- The first two arguments to CategorizedPlaintextCorpusReader are the root directory and fileids\n",
    "- (option 1) The `cat_pattern` keyword is passed to CategorizedCorpusReader, which overrides the common corpus reader functions such as fileids(), words(), sents(), and paras() to accept a categories keyword argument.\n",
    "- The CategorizedCorpusReader class provides the categories() function, which returns a list of all the known categories in the corpus.\n",
    "- (option 2) Instead of `cat_pattern`, we could pass in a `cat_map`, which is a dictionary mapping a fileid argument to a list of category labels\n",
    "- (option 3) A third way of specifying categories is to use the `cat_file` keyword argument to specify a filename containing a mapping of `fileid` to category. -> check the book\n",
    "\n",
    "### Categorized tagged corpus reader\n",
    "\n",
    "The brown corpus reader is actually an istance of `CategorizedTaggedCorpusReader`, which inherits from `CategorizedCorpusReader` and `TaggedCorpusReader`\n",
    "\n",
    "### Categorized corpora\n",
    "- The movie_reviews corpus reader is an instance of `CategorizedPlaintextCorpusReader`, as is the `reuters` corpus reader. \n",
    " - reuters has 90 catetories\n",
    " - movie_reviews has 2 categories\n",
    "- These corpora are often used for training and evaluating classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a categorized chunk corpus reader\n",
    "### Why do we have to create a categorized chunk corpus reader?\n",
    "- NLTK only provides a `CategorizedPlaintextCorpusReader` and `CategorizedTaggedCorpusReader` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus.reader import CategorizedCorpusReader, ChunkedCorpusReader\n",
    "\n",
    "class CategorizedChunkedCorpusReader(CategorizedCorpusReader, ChunkedCorpusReader):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        CategorizedCorpusReader.__init__(self, kwargs)\n",
    "        ChunkedCorpusReader.__init__(self, *args, **kwargs)\n",
    "    \n",
    "    def _resolve(self, fileids, categories):\n",
    "        if fileids is not None and categories is not None:\n",
    "            raise ValueError('Specify fileids or categories, not both')\n",
    "        if categories is not None:\n",
    "            return self.fileids(categories)\n",
    "        else:\n",
    "            return fileids\n",
    "    \n",
    "    def raw(self, fileids = None, categories = None):\n",
    "        return ChunkedCorpusReader.raw(self, self._resolve(fileids, categories))\n",
    "    \n",
    "    def words(self, fileids = None, categories = None):\n",
    "        return ChunkedCorpusReader.words(self, self._resolve(fileids, categories))\n",
    "    \n",
    "    def sents(self, fileids = None, categories = None):\n",
    "        return ChunkedCorpusReader.sents(self, self._resolve(fileids, categories))\n",
    "    \n",
    "    def paras(self, fileids = None, categories = None):\n",
    "        return ChunkedCorpusReader.paras(self, self._resolve(fileids, categories))\n",
    "    \n",
    "    def tagged_words(self, fileids = None, categories = None):\n",
    "        return ChunkedCorpusReader.tagged_words(self, self._resolve(fileids, categories))\n",
    "\n",
    "    def tagged_sents(self, fileids = None, categories = None):\n",
    "        return ChunkedCorpusReader.tagged_sents(self, self._resolve(fileids, categories))\n",
    "\n",
    "    def tagged_paras(self, fileids = None, categories = None):\n",
    "        return ChunkedCorpusReader.tagged_paras(self, self._resolve(fileids, categories))\n",
    "    \n",
    "    def chunked_words(self, fileids = None, categories = None):\n",
    "        return ChunkedCorpusReader.chunked_words(self, self._resolve(fileids, categories))\n",
    "\n",
    "    def chunked_sents(self, fileids = None, categories = None):\n",
    "        return ChunkedCorpusReader.chunked_sents(self, self._resolve(fileids, categories))\n",
    "\n",
    "    def chunked_paras(self, fileids = None, categories = None):\n",
    "        return ChunkedCorpusReader.chunked_paras(self, self._resolve(fileids, categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "path = nltk.data.find('corpora/treebank/tagged')\n",
    "reader = CategorizedChunkedCorpusReader(path, r'wsj_.*\\.pos', cat_pattern = r'wsj_(.*)\\.pos')\n",
    "print(len(reader.categories()) == len(reader.fileids()))\n",
    "print(len(reader.chunked_sents(categories=['0001'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus.reader import CategorizedCorpusReader, ConllCorpusReader, ConllChunkCorpusReader\n",
    "\n",
    "class CategorizedConllChunkCorpusReader(CategorizedCorpusReader, ConllChunkCorpusReader):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        CategorizedCorpusReader.__init__(self, kwargs)\n",
    "        ConllChunkCorpusReader.__init__(self, *args, **kwargs)\n",
    "    \n",
    "    def _resolve(self, fileids, categories):\n",
    "        if fileids is not None and categories is not None:\n",
    "            raise ValueError('Specify fileids or categories, not both')\n",
    "        if categories is not None:\n",
    "            return self.fileids(categories)\n",
    "        else:\n",
    "            return fileids\n",
    "    \n",
    "    def raw(self, fileids = None, categories = None):\n",
    "        return ConllCorpusReader.raw(self, self._resolve(fileids, categories))\n",
    "    \n",
    "    def words(self, fileids = None, categories = None):\n",
    "        return ConllCorpusReader.words(self, self._resolve(fileids, categories))\n",
    "    \n",
    "    def sents(self, fileids = None, categories = None):\n",
    "        return ConllCorpusReader.sents(self, self._resolve(fileids, categories))\n",
    "    \n",
    "    def tagged_words(self, fileids = None, categories = None):\n",
    "        return ConllCorpusReader.tagged_words(self, self._resolve(fileids, categories))\n",
    "    \n",
    "    def tagged_sents(self, fileids = None, categories = None):\n",
    "        return ConllCorpusReader.tagged_sents(self, self._resolve(fileids, categories))\n",
    "    \n",
    "    def chunked_words(self, fileids = None, categories = None, chunk_types = None):\n",
    "        return ConllCorpusReader.chunked_words(self, self._resolve(fileids, categories), chunk_types)\n",
    "    \n",
    "    def chunked_sents(self, fileids = None, categories = None, chunk_types = None):\n",
    "        return ConllCorpusReader.chunked_sents(self, self._resolve(fileids, categories), chunk_types)\n",
    "    \n",
    "    def parsed_sents(self, fileids = None, categories = None, pos_in_tree = None):\n",
    "        return ConllCorpusReader.parsed_sents(self, self._resolve(fileids, categories), pos_in_tree)\n",
    "    \n",
    "    def srl_spans(self, fileids = None, categories = None):\n",
    "        return ConllCorpusReader.srl_spans(self, self._resolve(fileids, categories))\n",
    "    \n",
    "    def srl_instances(self, fileids = None, categories = None, pos_in_tree = None, flatten = True):\n",
    "        return ConllCorpusReader.srl_instances(self, self._resolve(fileids, categories), pos_in_tree, flatten)\n",
    "    \n",
    "    def iob_words(self, fileids = None, categories = None):\n",
    "        return ConllCorpusReader.iob_words(self, self._resolve(fileids, categories))\n",
    "    \n",
    "    def iob_sents(self, fileids = None, categories = None):\n",
    "        return ConllCorpusReader.iob_sents(self, self._resolve(fileids, categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2012"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = nltk.data.find('corpora/conll2000')\n",
    "reader = CategorizedConllChunkCorpusReader(path, r'.*\\.txt', ('NP', 'VP', 'PP'), cat_pattern = r'(.*)\\.txt')\n",
    "reader.categories()\n",
    "reader.fileids()\n",
    "len(reader.chunked_sents(categories=['test']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lazy corpus loading\n",
    "\n",
    "- Loading a corpus reader can be an expensive operation due to the number of files, file sizes, and various initialization tasks.\n",
    "- Using LazyCorpusLoader to speed up module import time when a corpus reader is defined.\n",
    "\n",
    "### How to use LazyCorpusLoader?\n",
    "- It requires two arguments: the name of the corpus and the corpus reader class, plus any other arguments needed to initialize the corpus reader class\n",
    " - the `name` argument specifies the root directory name of the corpus, which must be within a corpora subdirectory of one of the paths in nltk.data.path\n",
    " - the corpus reader class `reader_cls` should be the name of a subclass of `CorpusReader`\n",
    "  - We will also need to pass in any other arguments required by the `reader_cls` argument for initialization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus.util import LazyCorpusLoader\n",
    "from nltk.corpus.reader import WordListCorpusReader\n",
    "reader = LazyCorpusLoader('cookbook', WordListCorpusReader, ['wordlist'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(reader, LazyCorpusLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wordlist']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(reader, LazyCorpusLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(reader, WordListCorpusReader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus.reader import BracketParseCorpusReader\n",
    "\n",
    "treebank = LazyCorpusLoader('treebank/combined', BracketParseCorpusReader, r'wsj_.*\\.mrg', tagset = 'wsj', encoding= 'ascii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus.reader import tagged_treebank_para_block_reader\n",
    "\n",
    "treebank_chunk = LazyCorpusLoader('treebank/tagged',\n",
    "                                  ChunkedCorpusReader,\n",
    "                                  r'wsj_.*\\.pos',\n",
    "                                  sent_tokenizer = RegexpTokenizer(r'(?<=/\\.)\\s*(?![^\\[]*\\])', gaps=True),\n",
    "                                  para_block_reader = tagged_treebank_para_block_reader,\n",
    "                                  encoding = 'ascii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus.reader import PlaintextCorpusReader\n",
    "\n",
    "treebank_raw = LazyCorpusLoader('treebank/raw',\n",
    "                                PlaintextCorpusReader, r'wsj_.*',\n",
    "                                encoding='ISO-8859-2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a custom corpus view\n",
    "\n",
    "### What is corpus views?\n",
    "- A `corpus view` is a class wrapper around a corpus file that reads in blocks of tokens as needed.\n",
    "- The purpose is to provide a view into a file without reading the whole file at once.\n",
    "\n",
    "### Other view choices\n",
    "#### Pickle corpus view\n",
    "- The `PickleCorpusView` can be found in nltk.corpus.reader.util.\n",
    "- check book page 79\n",
    "\n",
    "#### Concatenated corpus view\n",
    "- The `ConcatenatedCorpusView` class can be found in nltk.corpus.reader.util.\n",
    "- It's useful when we have multiple files that we want a corpus reader to treat as a single file.\n",
    "\n",
    "### Block reader functions\n",
    "- read_whitespace_block(): This will read 20 lines from the stream, splitting each line into tokens by whitespace.\n",
    "- read_wordpunct_block(): This reads 20 lines from the stream, splitting each line using nltk.tokenize.wordpunct_tokenize()\n",
    "- read_line_block(): This read 20 lines from the stream and returns them as a list, with each line as a token.\n",
    "- read_regexp_block(): This takes two additional arguments, which must be regular expressions that can be passed to re.match(): start_re and end_re.\n",
    " - The start_re variable matches the starting line of a block\n",
    " - The end_re variable matches the ending line of the block. \n",
    "  - default value is None\n",
    " - The return value is a single token of all lines in the block joined into a single string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original len is 4\n",
      "len after using IgnoreHeadingCorpusReader is 3\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus.reader import PlaintextCorpusReader\n",
    "from nltk.corpus.reader.util import StreamBackedCorpusView\n",
    "\n",
    "class IgnoreHeadingCorpusView(StreamBackedCorpusView):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        StreamBackedCorpusView.__init__(self, *args, **kwargs)\n",
    "        # open self._stream\n",
    "        self._open()\n",
    "        #skip the heading block\n",
    "        self.read_block(self._stream)\n",
    "        # reset the start position to the current position in the stream\n",
    "        self._filepos = [self._stream.tell()]\n",
    "\n",
    "class IgnoreHeadingCorpusReader(PlaintextCorpusReader):\n",
    "    CorpusView = IgnoreHeadingCorpusView\n",
    "    \n",
    "# example\n",
    "plain = PlaintextCorpusReader('corpora', ['heading_text.txt'])\n",
    "print('original len is', len(plain.paras()))\n",
    "reader = IgnoreHeadingCorpusReader('corpora', ['heading_text.txt'])\n",
    "print('len after using IgnoreHeadingCorpusReader is', len(reader.paras()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a MongoDB-backed corpus reader\n",
    "\n",
    "- require MongoDB and PyMongo\n",
    "\n",
    "---\n",
    "```\n",
    "import pymongo\n",
    "from nltk.data import LazyLoader\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.util import AbstractLazySequence, LazyMap, LazyConcatenation\n",
    "\n",
    "class MongoDBLazySequence(AbstractLazySequence):\n",
    "    def __init__(self, host='localhost', port=27017, db='test',\n",
    "        collection='corpus', field='text'):\n",
    "        self.conn = pymongo.MongoClient(host, port)\n",
    "        self.collection = self.conn[db][collection]\n",
    "        self.field = field\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.collection.count()\n",
    "\n",
    "    def iterate_from(self, start):\n",
    "        f = lambda d: d.get(self.field, '')\n",
    "        return iter(LazyMap(f, self.collection.find(fields=[self.field], skip=start)))\n",
    "\n",
    "class MongoDBCorpusReader(object):\n",
    "    def __init__(self, \n",
    "                 word_tokenizer=TreebankWordTokenizer(),   \n",
    "                 sent_tokenizer=LazyLoader('tokenizers/punkt/PY3 /english.pickle'), \n",
    "                 **kwargs):\n",
    "        self._seq = MongoDBLazySequence(**kwargs)\n",
    "        self._word_tokenize = word_tokenizer.tokenize\n",
    "        self._sent_tokenize = sent_tokenizer.tokenize\n",
    "\n",
    "    def text(self):\n",
    "        return self._seq\n",
    "\n",
    "    def words(self):\n",
    "        return LazyConcatenation(LazyMap(self._word_tokenize, self.text()))\n",
    "    \n",
    "    def sents(self):\n",
    "        return LazyConcatenation(LazyMap(self._sent_tokenize, self.text()))\n",
    "```\n",
    "---\n",
    "\n",
    "### How to use it?\n",
    "```\n",
    "reader = MongoDBCorpusReader(db = 'website',\n",
    "                             collection = 'comments',\n",
    "                             field = 'comment')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus editing with file locking\n",
    "\n",
    "- Corpus reader and views are all read-only, but there will be times when we want to add to or edit the corpus files.\n",
    "- In order to lock file, we have to install the `lockfile` library\n",
    "- Read page. 82 ~ 84\n",
    "\n",
    "---\n",
    "```\n",
    "import lockfile, tempfile, shutil\n",
    "    def append_line(fname, line): with lockfile.FileLock(fname):\n",
    "        fp = open(fname, 'a+')\n",
    "        fp.write(line)\n",
    "        fp.write('\\n')\n",
    "        fp.close()\n",
    "    def remove_line(fname, line):\n",
    "        with lockfile.FileLock(fname):\n",
    "            tmp = tempfile.TemporaryFile()\n",
    "            fp = open(fname, 'rw+')\n",
    "            # write all lines from orig file, except if matches given line\n",
    "            for l in fp:\n",
    "                if l.strip() != line:\n",
    "                    tmp.write(l)\n",
    "            # reset file pointers so entire files are copied\n",
    "            fp.seek(0)\n",
    "            tmp.seek(0)\n",
    "            # copy tmp into fp, then truncate to remove trailing line(s)\n",
    "            shutil.copyfileobj(tmp, fp)\n",
    "            fp.truncate()\n",
    "            fp.close()\n",
    "            tmp.close()\n",
    "```\n",
    "---\n",
    "\n",
    "### How to use it?\n",
    "```\n",
    "append_line('test.txt', 'foo')\n",
    "remove_line('test.txt', 'foo')\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
