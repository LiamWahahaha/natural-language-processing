{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-of-speech tagging\n",
    "\n",
    "- Default tagging\n",
    "- Training a unigram part-of-speech tagger\n",
    "- Combining taggers with backoff tagging\n",
    "- Training and combining ngram taggers\n",
    "- Creating a model of likely word tags\n",
    "- Tagging with regular expressions\n",
    "- Affix tagging\n",
    "- Training a Brill tagger\n",
    "- Training the TnT tagger\n",
    "- Using WordNet for tagging\n",
    "- Tagging proper names\n",
    "- Classifier-based tagging\n",
    "- Training a tagger with NLTK-Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is part-of-speech tagging?\n",
    "- It is the process of converting a sentence into a list of tuples\n",
    " - a sentence is in the form of a list of words\n",
    " - each tuple is of the form (word, tag)  \n",
    "   - The tag is a part-of-speech tag, signifies whether the word is a noun, adjective, verb, and so on\n",
    "\n",
    "## Why we need part-of-speech tagging?\n",
    "- It is a necessary step before chunking.\n",
    "- Without the part-of-speech tags, a chunker cannot know how to extract phrases from a sentence.\n",
    "\n",
    "### Reference\n",
    "- https://en.wikipedia.org/wiki/Word_sense_disambiguation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default tagging\n",
    "\n",
    "- This simply assigns the same part-of-speech tag to every token.\n",
    "- This tagger is useful as a last-resort tagger\n",
    "- It provides a baseline to measure accuracy improvements\n",
    "\n",
    "### Alternatives\n",
    "- Penn Treebank Part-of-speech tags\n",
    " - http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hello', 'NN'), ('World', 'NN')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import DefaultTagger\n",
    "tagger = DefaultTagger('NN') #NN, NP, ...\n",
    "tagger.tag(['Hello', 'World'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14331966328512843"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import treebank\n",
    "\n",
    "test_sents = treebank.tagged_sents()[3000:]\n",
    "tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tagging sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Hello', 'NN'), ('world', 'NN'), ('.', 'NN')],\n",
       " [('How', 'NN'), ('are', 'NN'), ('you', 'NN'), ('?', 'NN')]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger.tag_sents([['Hello', 'world', '.'], ['How', 'are', 'you', '?']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Untagging a tagged sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'World']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import untag\n",
    "\n",
    "untag([('Hello', 'NN'), ('World', 'NN')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traning a unigram part-of-speech tagger\n",
    "\n",
    "### Inheritance relationships\n",
    "- SequentialBackoffTagger\n",
    " - choose_tage()  \n",
    " \n",
    "- ContextTagger(SequentialBackoffTagger)\n",
    " - context()\n",
    "\n",
    "- AffixTagger(ContextTagger)\n",
    "- NgramTagger(ContextTagger)\n",
    " - UnigramTagger(NgramTagger)\n",
    " - BigramTagger(NgramTagger)\n",
    " - TrigramTagger(NgramTagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pierre',\n",
       " 'Vinken',\n",
       " ',',\n",
       " '61',\n",
       " 'years',\n",
       " 'old',\n",
       " ',',\n",
       " 'will',\n",
       " 'join',\n",
       " 'the',\n",
       " 'board',\n",
       " 'as',\n",
       " 'a',\n",
       " 'nonexecutive',\n",
       " 'director',\n",
       " 'Nov.',\n",
       " '29',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import UnigramTagger\n",
    "from nltk.corpus import treebank\n",
    "\n",
    "train_sents = treebank.tagged_sents()[:3000]\n",
    "tagger = UnigramTagger(train_sents)\n",
    "treebank.sents()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Pierre', 'NNP'),\n",
       " ('Vinken', 'NNP'),\n",
       " (',', ','),\n",
       " ('61', 'CD'),\n",
       " ('years', 'NNS'),\n",
       " ('old', 'JJ'),\n",
       " (',', ','),\n",
       " ('will', 'MD'),\n",
       " ('join', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('board', 'NN'),\n",
       " ('as', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('nonexecutive', 'JJ'),\n",
       " ('director', 'NN'),\n",
       " ('Nov.', 'NNP'),\n",
       " ('29', 'CD'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treebank.tagged_sents()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8571551910209367"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overriding the context model\n",
    "- All taggers that inherit from `ContextTagger` can take a pre-built model instead of training their own.\n",
    "- The pre-built model is a Python dict mapping a context key to a tag\n",
    "\n",
    "### Use case\n",
    "- Unless we know exactly what we are doing, let the tagger train its own model instead of passing in our own.\n",
    "- A good case for passing a self-created model to the `UnigramTagger` class is for when we have a dictionary of words and tags, and we know that every word should always map to its tag.\n",
    " - We can put this `UnigramTagger` as our first backoff tagger.\n",
    "\n",
    "### Minimum frequency cutoff\n",
    "- The `ContextTagger` class uses frequency of occurrence to decide which tag is most likely for a given context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Pierre', 'NN'),\n",
       " ('Vinken', None),\n",
       " (',', None),\n",
       " ('61', None),\n",
       " ('years', None),\n",
       " ('old', None),\n",
       " (',', None),\n",
       " ('will', None),\n",
       " ('join', None),\n",
       " ('the', None),\n",
       " ('board', None),\n",
       " ('as', None),\n",
       " ('a', None),\n",
       " ('nonexecutive', None),\n",
       " ('director', None),\n",
       " ('Nov.', None),\n",
       " ('29', None),\n",
       " ('.', None)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger = UnigramTagger(model = {'Pierre': 'NN'})\n",
    "tagger.tag(treebank.sents()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.775350744657889"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger = UnigramTagger(train_sents, cutoff = 3)\n",
    "tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining taggers with backoff tagging\n",
    "\n",
    "- `Backoff tagging` is one of the core features of `SequentialBackoffTagger`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8741204403194475"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger1 = DefaultTagger('NN')\n",
    "tagger2 = UnigramTagger(train_sents, backoff = tagger1)\n",
    "tagger2.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and loading a trained tagger with pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "f = open('tagger.pickle', 'wb')\n",
    "pickle.dump(tagger, f)\n",
    "f.close()\n",
    "f = open('tagger.pickle', 'rb')\n",
    "tagger = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.775350744657889"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and combining ngram taggers\n",
    "\n",
    "- BigramTagger\n",
    "- TrigramTagger\n",
    "- NgramTagger\n",
    " - e.g. quadgram tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11318799913662854"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import BigramTagger, TrigramTagger\n",
    "bitagger = BigramTagger(train_sents)\n",
    "bitagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06902654867256637"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tritagger = TrigramTagger(train_sents)\n",
    "tritagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backoff_tagger(train_sent, tagger_classes, backoff = None):\n",
    "    for cls in tagger_classes:\n",
    "        backoff = cls(train_sents, backoff = backoff)\n",
    "    \n",
    "    return backoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8806388948845241"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backoff = DefaultTagger('NN')\n",
    "tagger = backoff_tagger(train_sents, [UnigramTagger, BigramTagger, TrigramTagger], backoff = backoff)\n",
    "tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<TrigramTagger: size=845>,\n",
       " <BigramTagger: size=1859>,\n",
       " <UnigramTagger: size=8818>,\n",
       " <DefaultTagger: tag=NN>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger._taggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(tagger._taggers[0], TrigramTagger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quadgram tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import NgramTagger\n",
    "# quadtagger = NgramTagger(4, train_sents)\n",
    "# quadtagger.evaluate(test_sents)\n",
    "\n",
    "class QuadgramTagger(NgramTagger):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        NgramTagger.__init__(self, 4, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8805093891646881"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quadtagger = backoff_tagger(train_sents, [UnigramTagger, BigramTagger, TrigramTagger, QuadgramTagger], backoff = backoff)\n",
    "quadtagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a model of likely word tags\n",
    "\n",
    "- The word_tag_model() function take three arguments:\n",
    " - a list of all words\n",
    " - a list of all tagged words\n",
    " - the maximum number of words we want to use for our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist, ConditionalFreqDist\n",
    "\n",
    "def word_tag_model(words, tagged_words, limit = 200):\n",
    "    fd = FreqDist(words)\n",
    "    cfd = ConditionalFreqDist(tagged_words)\n",
    "    most_freq = (word for word, count in fd.most_common(limit))\n",
    "    return dict((word, cfd[word].max()) for word in most_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5593352039715087"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import treebank\n",
    "\n",
    "model = word_tag_model(treebank.words(), treebank.tagged_words())\n",
    "tagger = UnigramTagger(model = model)\n",
    "tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8806388948845241"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_tagger = DefaultTagger('NN')\n",
    "likely_tagger = UnigramTagger(model = model, backoff = default_tagger)\n",
    "tagger = backoff_tagger(train_sents, [UnigramTagger, BigramTagger, TrigramTagger], backoff = likely_tagger)\n",
    "tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8817181092164904"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger = backoff_tagger(train_sents, [UnigramTagger, BigramTagger, TrigramTagger], backoff = default_tagger)\n",
    "likely_tagger = UnigramTagger(model = model, backoff = tagger)\n",
    "likely_tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tagging with regular expressions\n",
    "\n",
    "Can use this tagger as backoff tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [\n",
    "    (r'^\\d+$', 'CD'), # Cardinal number\n",
    "    (r'.*ing$', 'VBG'), # gerunds, i.e. wondering\n",
    "    (r'.*ment$', 'NN'), # i.e. wonderment\n",
    "    (r'.*ful$', 'JJ'), # i.e. wonderful\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.037470321605870924"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import RegexpTagger\n",
    "\n",
    "tagger = RegexpTagger(patterns)\n",
    "tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affix tagging\n",
    "\n",
    "- The AffixTagger class is another `ContextTagger` subclass\n",
    "\n",
    "### Working with min_stem_length\n",
    "- The AffixTagger class also takes a min_stem_length keyword argument\n",
    " - The default value is 2.\n",
    " - If the word length is less than min_stem_length plus the absolute value of affix_length, then None is returned by the context() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27507014893157783"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import AffixTagger\n",
    "tagger = AffixTagger(train_sents)\n",
    "tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2365637815670192"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix_tagger = AffixTagger(train_sents, affix_length = 3)\n",
    "prefix_tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3196201165551478"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suffix_tagger = AffixTagger(train_sents, affix_length = -2)\n",
    "suffix_tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Brill tagger\n",
    "\n",
    "### What is a Brill tagger?\n",
    "- brill.Template(brill.Pos([-1])) means that a rule can be generated using the previous part-of-speech tag\n",
    "- brill.Template(brill.Pos([1])) means that we can look at the next part-of-speech tag to generate a rule\n",
    "- brill.Template(brill.Word([-2, -1])) means we can look at the combination of the previous two words to learn a transformation rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import brill, brill_trainer\n",
    "\n",
    "def train_brill_tagger(initial_tagger, train_sents, **kwargs):\n",
    "    templates = [\n",
    "        brill.Template(brill.Pos([-1])),\n",
    "        brill.Template(brill.Pos([1])),\n",
    "        brill.Template(brill.Pos([-2])),\n",
    "        brill.Template(brill.Pos([2])),\n",
    "        brill.Template(brill.Pos([-2, -1])),\n",
    "        brill.Template(brill.Pos([1, 2])),\n",
    "        brill.Template(brill.Pos([-3, -2, -1])),\n",
    "        brill.Template(brill.Pos([1, 2, 3])),\n",
    "        brill.Template(brill.Pos([-1]), brill.Pos([1])),\n",
    "        brill.Template(brill.Word([-1])),\n",
    "        brill.Template(brill.Word([1])),\n",
    "        brill.Template(brill.Word([-2])),\n",
    "        brill.Template(brill.Word([2])),\n",
    "        brill.Template(brill.Word([-2, -1])),\n",
    "        brill.Template(brill.Word([1, 2])),\n",
    "        brill.Template(brill.Word([-3, -2, -1])),\n",
    "        brill.Template(brill.Word([1, 2, 3])),\n",
    "        brill.Template(brill.Word([-1]), brill.Word([1])),\n",
    "    ]\n",
    "    \n",
    "    trainer = brill_trainer.BrillTaggerTrainer(initial_tagger, templates, deterministic = True)\n",
    "\n",
    "    return trainer.train(train_sents, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8806388948845241"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_tagger = DefaultTagger('NN')\n",
    "initial_tagger = backoff_tagger(train_sents, [UnigramTagger, BigramTagger, TrigramTagger], backoff = default_tagger)\n",
    "initial_tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8822361320958342"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brill_tagger = train_brill_tagger(initial_tagger, train_sents)\n",
    "brill_tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the TnT tagger\n",
    "\n",
    "- TnT stands for Trigram'nTags.\n",
    "- This is a statistical tagger based on second order Markov models\n",
    "- http://www.coli.uni-saarland.de/~thorsten/tnt/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.875545003237643"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import tnt\n",
    "tnt_tagger = tnt.TnT()\n",
    "tnt_tagger.train(train_sents)\n",
    "tnt_tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TnT()\n",
    "- The TnT tagger accepts a few optional keyword arguments.\n",
    "- We can pass in a tagger for unknown words as unk\n",
    "- If the tagger is already trained, then we must also pass in Trained = True, otherwise it will call unk.train(data) with the same data we pass into the train() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.892467083962875"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unk = DefaultTagger('NN')\n",
    "tnt_tagger = tnt.TnT(unk = unk, Trained = True)\n",
    "tnt_tagger.train(train_sents)\n",
    "tnt_tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Controlling the beam search\n",
    "\n",
    "- `N` parameter controls the number of possible solutions the tagger maintains while trying to guess the tags for a sentence.\n",
    "- The default value of N is 1000.\n",
    "- N can influence both memory and accuracy.\n",
    "- Don't assume that accuracy will not change if we decrease N.\n",
    "- Experiment with our own data to be sure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.875545003237643"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tnt_tagger = tnt.TnT(N = 100)\n",
    "tnt_tagger.train(train_sents)\n",
    "tnt_tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Significance of capitalization\n",
    "\n",
    "- We can pass `C = True` to the TnT constructor if we want capitalization of words to be significant.\n",
    "- The default is C = False."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using WordNet for tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18737985576240793"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import SequentialBackoffTagger\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "class WordNetTagger(SequentialBackoffTagger):\n",
    "    '''\n",
    "    >>> wt = WordNetTagger()\n",
    "    >>> wt.tag(['food', 'is', 'great'])\n",
    "    [('food', 'NN'), ('is', 'VB'), ('great', 'JJ')]\n",
    "    '''\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        SequentialBackoffTagger.__init__(self, *args, **kwargs)\n",
    "        \n",
    "        self.wordnet_tag_map = {\n",
    "            'n': 'NN',\n",
    "            's': 'JJ',\n",
    "            'a': 'JJ',\n",
    "            'r': 'RB',\n",
    "            'v': 'VB'\n",
    "        }\n",
    "    \n",
    "    def choose_tag(self, tokens, index, history):\n",
    "        word = tokens[index]\n",
    "        fd = FreqDist()\n",
    "        \n",
    "        for synset in wordnet.synsets(word):\n",
    "            fd[synset.pos()] += 1\n",
    "\n",
    "        try:\n",
    "            return self.wordnet_tag_map.get(fd.max())\n",
    "        except:\n",
    "            return self.wordnet_tag_map['n']\n",
    "\n",
    "wn_tagger = WordNetTagger()\n",
    "wn_tagger.evaluate(train_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8865098208504208"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import UnigramTagger, BigramTagger, TrigramTagger\n",
    "\n",
    "tagger = backoff_tagger(train_sents, [UnigramTagger, BigramTagger, TrigramTagger], backoff = wn_tagger)\n",
    "tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tagging proper names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Jacob', 'NNP')]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import SequentialBackoffTagger\n",
    "from nltk.corpus import names\n",
    "\n",
    "class NamesTagger(SequentialBackoffTagger):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        SequentialBackoffTagger.__init__(self, *args, **kwargs)\n",
    "        self.name_set = set([n.lower() for n in names.words()])\n",
    "        \n",
    "    def choose_tag(self, tokens, index, history):\n",
    "        word = tokens[index]\n",
    "\n",
    "        if word.lower() in self.name_set:\n",
    "            return 'NNP'\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "nt = NamesTagger()\n",
    "nt.tag(['Jacob'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier-based tagging\n",
    "\n",
    "- The `ClassifierBasedPOSTagger` class uses classification to do part-of-speech tagging.\n",
    " - A subclass of `ClassifierBasedTagger`\n",
    " - This tagger implements a feature detector that combines many of the techniques of the previous taggers into a single feature set.\n",
    " - The feature detector  \n",
    "    - finds multiple length suffixes\n",
    "    - does some regular expression matching\n",
    "    - looks at the unigram, bigram, and trigram history\n",
    " - We can use a different classifier instead of `NaiveBayesClassifier` by passing in our own `classifier_builder` function\n",
    "- The `ClassifierBasedTagger` class is often the most accurate tagger, but it's also one of the slowest taggers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9309734513274336"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag.sequential import ClassifierBasedPOSTagger\n",
    "\n",
    "tagger = ClassifierBasedPOSTagger(train = train_sents)\n",
    "tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -3.82864        0.008\n",
      "             2          -0.76859        0.957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/nltk/classify/maxent.py:1392: RuntimeWarning: overflow encountered in power\n",
      "  exp_nf_delta = 2 ** nf_delta\n",
      "/usr/local/lib/python3.7/site-packages/nltk/classify/maxent.py:1394: RuntimeWarning: invalid value encountered in multiply\n",
      "  sum1 = numpy.sum(exp_nf_delta * A, axis=0)\n",
      "/usr/local/lib/python3.7/site-packages/nltk/classify/maxent.py:1395: RuntimeWarning: invalid value encountered in multiply\n",
      "  sum2 = numpy.sum(nf_exp_nf_delta * A, axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Final               nan        0.984\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9258363911072739"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.classify import MaxentClassifier\n",
    "\n",
    "me_tagger = ClassifierBasedPOSTagger(train = train_sents,\n",
    "                                     classifier_builder = MaxentClassifier.train)\n",
    "me_tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting features with a custom feature detector\n",
    "- If we want to do our own feature detection, there are two ways to do it:\n",
    "  1. Subclass `ClassifierBasedTagger` and implement a `feature_detector()` method  \n",
    "  2. Pass a function as the `feature_detector keyword` argument into `ClassifierBasedTagger` at initialization  \n",
    "- Either way, we need a feature detection method that can take the same arguments as `choose_tag(): `tokens, index, history  \n",
    " - return a `dict` of key-value features  \n",
    "   - key is the feature name\n",
    "   - value is the feature value\n",
    "\n",
    "### Setting a cutoff probability\n",
    "If we want to use a backoff tagger, we have to pass in a `cutoff_prob` argument to specify the probability threshold for classification. Then, if the probability of the chosen tag is less than cutoff_prob, the backoff tagger will be used.\n",
    "\n",
    "### Using a pre-trained classifier\n",
    "If we want to use a classifier that's already been trained, then we can pass that into `ClassifierBasedTagger` or `ClassifierBasedPOSTagger` as the `classifier`.\n",
    "- In this case, the `classifier_builder` argument is ignored and no training takes place.\n",
    "- We must ensure that the classifier has been trained on and can classify feature sets produced by whatever `feature_detector()` methoud we use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8733865745737104\n",
      "0.9311029570472696\n"
     ]
    }
   ],
   "source": [
    "def unigram_feature_detector(tokens, index, history):\n",
    "    return {'word': tokens[index]}\n",
    "\n",
    "from nltk.tag.sequential import ClassifierBasedTagger\n",
    "tagger = ClassifierBasedTagger(train = train_sents,\n",
    "                               feature_detector = unigram_feature_detector)\n",
    "\n",
    "print(tagger.evaluate(test_sents))\n",
    "\n",
    "default = DefaultTagger('NN')\n",
    "tagger = ClassifierBasedPOSTagger(train = train_sents,\n",
    "                                  backoff = default,\n",
    "                                  cutoff_prob = 0.3)\n",
    "print(tagger.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a tagger with NLTK-Trainer\n",
    "- NLTK-Trainer is a collection of scripts that give us the ability to run training experiements without writing a single line of code.\n",
    "- Project link: https://github.com/japerk/nltk-trainer\n",
    "- Documentation: http://nltk-trainer.readthedocs.org/\n",
    "\n",
    "### NLTK-Trainer provides a script that can save a pickled tagger (train_tagger.py)\n",
    "### NLTK-Trainer provides a script that can train on a custom corpus (train_tagger.py)\n",
    "### NLTK-Trainer provides a script that can train with universal tags (train_tagger.py)\n",
    "### NLTK-Trainer provides a script that can analyze a tagger against a tagged corpus (analyze_tagger_coverage.py)\n",
    "### NLTK-Trainer provides a script that can analyze a tagged corpus (analyze_tagged_corpus.py)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
