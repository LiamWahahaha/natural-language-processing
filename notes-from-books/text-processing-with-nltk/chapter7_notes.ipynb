{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text classification\n",
    "- Bag of words feature extraction\n",
    "- Training a Naive Bayes classifier\n",
    "- Training a decision tree classifier\n",
    "- Training a maximum entropy classifier\n",
    "- Training scikit-learn classifiers\n",
    "- Measuring precision and recall of a classifier\n",
    "- Calculating high information words\n",
    "- Combining classifiers with voting\n",
    "- Classifying with multiple binary classifiers\n",
    "- Training a classifier with NLTK-Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words feature extraction\n",
    "\n",
    "- The `bag of words` model is the simplest method\n",
    "- It constructs a word presence feature set from all the words of an instance\n",
    "- This method doesn't care about the order of the words\n",
    "- This method doesn't care about how many times a word occurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(words):\n",
    "    return dict([(word, True) for word in words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to improve the bag of word method? We can...\n",
    "\n",
    "- Construct a set of words that we want to exclude\n",
    " - Filtering stopwords\n",
    "- Including significant bigrams\n",
    " - In addition to single words, it often helps to include significant bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'brown': True, 'quick': True, 'fox': True}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def bag_of_words_not_in_set(words, badwords):\n",
    "    return bag_of_words(set(words) - set(badwords))\n",
    "\n",
    "def bag_of_non_stopwords(words, stopfile = 'english'):\n",
    "    badwords = stopwords.words(stopfile)\n",
    "    return bag_of_words_not_in_set(words, badwords)\n",
    "\n",
    "\n",
    "bag_of_non_stopwords(word_tokenize(\"the quick brown fox\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': True,\n",
       " 'quick': True,\n",
       " 'brown': True,\n",
       " 'fox': True,\n",
       " ('brown', 'fox'): True,\n",
       " ('quick', 'brown'): True,\n",
       " ('the', 'quick'): True}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "\n",
    "def bag_of_bigrams_words(words, score_fn = BigramAssocMeasures.chi_sq, n = 200):\n",
    "    bigram_finder = BigramCollocationFinder.from_words(words)\n",
    "    bigrams = bigram_finder.nbest(score_fn, n)\n",
    "    return bag_of_words(words + bigrams)\n",
    "\n",
    "bag_of_bigrams_words(word_tokenize(\"the quick brown fox\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Naive Bayes classifier\n",
    "\n",
    "- Use the `Bayes theorem` to predict the probability that a given feature set belongs to a particular label.\n",
    " - The formula is:  \n",
    "   P(label | features) = P(label) * P(features | label) / P(features)  \n",
    "   > P(label):  \n",
    "       The prior probability of the label occurring, which is the likelihood that a random feature set will have the label.  \n",
    "     P(features | label):  \n",
    "       The prior probability of a given feature set being classified as that label.  \n",
    "     P(features):  \n",
    "       The prior probability of a given feature set occurring.  \n",
    "     P(label | features):  \n",
    "       This tells us the probability that the given features should have that label.\n",
    "- `The split_label_feats should modify.`\n",
    "- During traing, the `NaiveBayesClassifier` class constructs probability distributions for each feature using an `estimator` parameter.  \n",
    " - Default estimator: nltk.probability.ELEProbDist  \n",
    "  - ELE stands for Expected Likelihood Estimate\n",
    "  - The formula for calculating the label probabilities for a given feature is (`c` + 0.5) / (`N` + `B` / 2)  \n",
    "   > `c`: count of times a single feature occurs  \n",
    "     `N`: the total number of feature outcomes observed  \n",
    "     `B`: the number of bins or unique features in the feature set  \n",
    " \n",
    " - Other choices  \n",
    "  - we can use any `estimator` parameter we want, and there are quite a few to choose from.\n",
    "  - The only constraints are that it must inherit from `nltk.probability.ProbDistI` and its constructor must take a `bins` keyword argument.\n",
    "  - For example:\n",
    "   - Using LacplaceProdDist class instead, which uses the formula (`c` + 1) / (`N` + `B`)  \n",
    "   ```\n",
    "   from nltk.probability import LaplaceProbDist  \n",
    "   nb_classifier = NaiveBayesClassifier.train(train_feats, estimator = LaplaceProbDist)  \n",
    "   accuracy(nb_classifier, test_feats)\n",
    "   ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "def label_feats_from_corpus(corp, feature_detector = bag_of_words):\n",
    "    label_feats = collections.defaultdict(list)\n",
    "    for label in corp.categories():\n",
    "        for fileid in corp.fileids(categories = [label]):\n",
    "            feats = feature_detector(corp.words(fileids = [fileid]))\n",
    "            label_feats[label].append(feats)\n",
    "    return label_feats\n",
    "\n",
    "def split_label_feats(lfeats, split = 0.75):\n",
    "    train_feats = list()\n",
    "    test_feats = list()\n",
    "    for label, feats in lfeats.items():\n",
    "        cutoff = int(len(feats)* split)\n",
    "        train_feats.extend([(feat, label) for feat in feats[:cutoff]])\n",
    "        test_feats.extend([(feat, label) for feat in feats[cutoff:]])\n",
    "    return train_feats, test_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This Naive Bayes classifier has labels:  ['neg', 'pos']\n",
      "comment:  the plot was ludicrous\n",
      "predicted category:  neg\n",
      "comment:  kate winslet is accessible\n",
      "predicted category:  pos\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.728"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.classify.util import accuracy\n",
    "\n",
    "movie_reviews.categories()\n",
    "lfeats = label_feats_from_corpus(movie_reviews)\n",
    "lfeats.keys()\n",
    "train_feats, test_feats = split_label_feats(lfeats, split = 0.75)\n",
    "nb_classifier = NaiveBayesClassifier.train(train_feats)\n",
    "print('This Naive Bayes classifier has labels: ', nb_classifier.labels())\n",
    "\n",
    "comments = [\"the plot was ludicrous\", \"kate winslet is accessible\"]\n",
    "for comment in comments:\n",
    "    print('comment: ', comment)\n",
    "    print('predicted category: ', nb_classifier.classify(bag_of_words(word_tokenize(comment))))\n",
    "\n",
    "# test the accuracy\n",
    "accuracy(nb_classifier, test_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob[neg] = 0.0000000354\n",
      "prob[pos] = 0.9999999646\n"
     ]
    }
   ],
   "source": [
    "probs = nb_classifier.prob_classify(test_feats[0][0])\n",
    "probs.samples()\n",
    "for key in probs.samples():\n",
    "    print('prob[{}] = {:.10f}'.format(key, probs.prob(key)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most informative features\n",
    "\n",
    "- most_informative_features()\n",
    "- show_most_informative_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "             magnificent = True              pos : neg    =     15.0 : 1.0\n",
      "             outstanding = True              pos : neg    =     13.6 : 1.0\n",
      "               insulting = True              neg : pos    =     13.0 : 1.0\n",
      "              vulnerable = True              pos : neg    =     12.3 : 1.0\n",
      "               ludicrous = True              neg : pos    =     11.8 : 1.0\n",
      "             uninvolving = True              neg : pos    =     11.7 : 1.0\n",
      "                  avoids = True              pos : neg    =     11.7 : 1.0\n",
      "              astounding = True              pos : neg    =     10.3 : 1.0\n",
      "             fascination = True              pos : neg    =     10.3 : 1.0\n",
      "                 idiotic = True              neg : pos    =      9.8 : 1.0\n"
     ]
    }
   ],
   "source": [
    "nb_classifier.most_informative_features(n = 10)\n",
    "nb_classifier.show_most_informative_features(n = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative: Manual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import DictionaryProbDist\n",
    "\n",
    "label_probdist = DictionaryProbDist({'pos': 0.5, 'neg': 0.5})\n",
    "true_probdist = DictionaryProbDist({True: 1})\n",
    "feature_probdist = {('pos', 'yes'): true_probdist, ('neg', 'no'): true_probdist}\n",
    "classifier = NaiveBayesClassifier(label_probdist, feature_probdist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a decision tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.678"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.classify import DecisionTreeClassifier\n",
    "dt_classifier = DecisionTreeClassifier.train(train_feats,\n",
    "                                             binary = True,\n",
    "                                             entropy_cutoff = 0.8,\n",
    "                                             depth_cutoff = 5,\n",
    "                                             support_cutoff = 30)\n",
    "accuracy(dt_classifier, test_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to use the `DecisionTreeClassifier.train()` class methld?\n",
    "#### Tell the classifier is a binary classifier or not\n",
    "- If the classifier only chooses between two labels, it is a binary classifier\n",
    "- If we have multivalued features, we will want to stick to the default `binary = False`\n",
    "\n",
    "#### Controlling uncertainty with `entropy_cutoff`\n",
    "- Entropy is the uncertainty of the outcome.  \n",
    " - As entropy approaches 1.0, uncertainty increases  \n",
    " - As entropy approaches 0.0, uncertainty decreases\n",
    "\n",
    "- The entropy_cutoff value is used during the tree refinement process  \n",
    "- If the entropy of the probability distribution of label choices in the tree is greater than the entropy_cutoff value, then the tree is refined further by creating more branches.  \n",
    "- If the entropy is lower than the entropy_cutoff value, then tree refinement is halted.  \n",
    "- Entropy is calculated by giving nltk.probability.entropy() a MLEProbDist value created from a FreqDist of label counts.\n",
    "\n",
    "#### Controlling tree depth with `depth_cutoff`\n",
    "- The final decision tree will never be deeper than the deepth_cutoff value  \n",
    " - Default value is 100\n",
    "\n",
    "#### Controlling decisions with `support_cutoff`\n",
    "- The suppor_cutoff values controls how many labeled feature sets are required to refine the tree\n",
    "- Labeled feature sets are eliminated once they no longer provide value to the training process\n",
    "- When the number of labeled feature sets is less than or equal to support_cutoff, refinement stops, at least for that section of the tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a maximum entropy classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a scikit-learn classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring precision and recall of a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating high information words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining classifiers with voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying with multiple binary classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a classifier with NLTK-Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
