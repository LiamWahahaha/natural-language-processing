{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing Text and Wordnet basics \n",
    "\n",
    "- practice based on Jacob Perkins-Python 3 Text Processing with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Given an array A of distinct integers sorted in ascending order, return the smallest index i that satisfies A[i] == i.', 'Return -1 if no such i exists.']\n",
      "['Given an array A of distinct integers sorted in ascending order, return the smallest index i that satisfies A[i] == i.', 'Return -1 if no such i exists.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "para = \"Given an array A of distinct integers sorted in ascending order, return the smallest index i that satisfies A[i] == i.  Return -1 if no such i exists.\"\n",
    "print(sent_tokenize(para))\n",
    "# if we're going to be tokenizing a lot of sentences, it's more efficient to load the PunktSentenceTokenizer class once, and call its tokenize() method instead:\n",
    "import nltk.data\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/PY3/english.pickle')\n",
    "print(tokenizer.tokenize(para))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'World', '.']\n",
      "['Hello', 'World', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "print(word_tokenize('Hello World.'))\n",
    "\n",
    "# this is equivalent to the following code\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "print(tokenizer.tokenize('Hello World.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Can', \"'\", 't', 'is', 'a', 'contraction', '.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "tokenizer.tokenize(\"Can't is a contraction.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ca', \"n't\", 'is', 'a', 'contraction', '.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(\"Can't is a contraction.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PunktWordTokenizer\n",
    "\n",
    "There is no PunktWordTokenizer. \n",
    "There is only sentence level tokenizer: PunktSentenceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Can't is a contraction.\"]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "tokenizer = PunktSentenceTokenizer()\n",
    "tokenizer.tokenize(\"Can't is a contraction.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokening sentences using regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Can't\", 'is', 'a', 'contraction']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "tokenizer.tokenize(\"Can't is a contraction.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple whitespace tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Can't\", 'is', 'a', 'contraction.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer('\\s+', gaps=True)\n",
    "tokenizer.tokenize(\"Can't is a contraction.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a sentence tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained sentence tokenizer\n",
      "White guy: So, do you have any plans for this evening?\n",
      "Girl: But you already have a Big Mac...\n",
      "\n",
      "Original sentence tokenizer\n",
      "White guy: So, do you have any plans for this evening?\n",
      "Girl: But you already have a Big Mac...\n",
      "Hobo: Oh, this is all theatrical.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from nltk.corpus import webtext\n",
    "text = webtext.raw('overheard.txt')\n",
    "sent_tokenizer = PunktSentenceTokenizer(text)\n",
    "\n",
    "sents1 = sent_tokenizer.tokenize(text)\n",
    "print(\"Trained sentence tokenizer\")\n",
    "print(sents1[0])\n",
    "print(sents1[678])\n",
    "\n",
    "# compare it with original sent_tokenize\n",
    "print(\"\\nOriginal sentence tokenizer\")\n",
    "sents2 = sent_tokenize(text)\n",
    "print(sents2[0])\n",
    "print(sents2[678])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering stopwords in a tokenized sentence\n",
    "\n",
    "* The stopwords corpus is an instance of nltk.corpus.reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Can't\", 'contraction.']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "english_stops = set(stopwords.words('english'))\n",
    "words = tokenizer.tokenize(\"Can't is a contraction.\")\n",
    "[word for word in words if word not in english_stops]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arabic',\n",
       " 'azerbaijani',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'greek',\n",
       " 'hungarian',\n",
       " 'indonesian',\n",
       " 'italian',\n",
       " 'kazakh',\n",
       " 'nepali',\n",
       " 'norwegian',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'spanish',\n",
       " 'swedish',\n",
       " 'turkish']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking up Synsets for a word in WordNet\n",
    "\n",
    "WordNet is a lexical database for the English language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cookbook.n.01\n",
      "a book of recipes and cooking directions\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "syn = wordnet.synsets('cookbook')[0]\n",
    "print(syn.name())\n",
    "print(syn.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cooking can be a great art',\n",
       " 'people are needed who have experience in cookery',\n",
       " 'he left the preparation of meals to his wife']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.synsets('cooking')[0].examples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with hypernyms\n",
    "\n",
    "Synsets are organized in a structure similar to that of an inheritance tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('reference_book.n.01')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn.hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('annual.n.02'),\n",
      " Synset('atlas.n.02'),\n",
      " Synset('cookbook.n.01'),\n",
      " Synset('directory.n.01'),\n",
      " Synset('encyclopedia.n.01'),\n",
      " Synset('handbook.n.01'),\n",
      " Synset('instruction_book.n.01'),\n",
      " Synset('source_book.n.01'),\n",
      " Synset('wordbook.n.01')]\n"
     ]
    }
   ],
   "source": [
    "pprint(syn.hypernyms()[0].hyponyms())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('entity.n.01')]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn.root_hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Synset('entity.n.01'),\n",
       "  Synset('physical_entity.n.01'),\n",
       "  Synset('object.n.01'),\n",
       "  Synset('whole.n.02'),\n",
       "  Synset('artifact.n.01'),\n",
       "  Synset('creation.n.02'),\n",
       "  Synset('product.n.02'),\n",
       "  Synset('work.n.02'),\n",
       "  Synset('publication.n.01'),\n",
       "  Synset('book.n.01'),\n",
       "  Synset('reference_book.n.01'),\n",
       "  Synset('cookbook.n.01')]]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn.hypernym_paths()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of speech (POS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'n'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn.pos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordnet.synsets('great'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['n', 's', 's', 's', 's', 's', 's']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# great has 1 noun Synset and 6 adjective Synsets\n",
    "print(len(wordnet.synsets('great', pos='n')))\n",
    "print(len(wordnet.synsets('great', pos='a')))\n",
    "[word.pos() for word in wordnet.synsets('great')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['n', 'n', 'n', 'n']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fun has 4 noun Synset\n",
    "print(len(wordnet.synsets('fun', pos='n')))\n",
    "print(len(wordnet.synsets('fun', pos='a')))\n",
    "print(len(wordnet.synsets('fun', pos='r')))\n",
    "print(len(wordnet.synsets('fun', pos='v')))\n",
    "[word.pos() for word in wordnet.synsets('fun')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking up lemmas and synonyms in WordNet\n",
    "\n",
    "- A lemma (in linguistics), is the canonical form or morphological form of a word.\n",
    "- Some lemmas also have antonyms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "cookbook\n",
      "cookery_book\n",
      "38 25\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "syn = wordnet.synsets('cookbook')[0]\n",
    "lemmas = syn.lemmas()\n",
    "print(len(lemmas))\n",
    "\n",
    "for lemma in lemmas:\n",
    "    print(lemma.name())\n",
    "\n",
    "# print all possible synonyms\n",
    "synonyms = []\n",
    "for syn in wordnet.synsets('book'):\n",
    "    for lemma in syn.lemmas():\n",
    "        synonyms.append(lemma.name())\n",
    "print(len(synonyms), len(set(synonyms)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['book',\n",
       " 'book',\n",
       " 'volume',\n",
       " 'record',\n",
       " 'record_book',\n",
       " 'book',\n",
       " 'script',\n",
       " 'book',\n",
       " 'playscript',\n",
       " 'ledger',\n",
       " 'leger',\n",
       " 'account_book',\n",
       " 'book_of_account',\n",
       " 'book',\n",
       " 'book',\n",
       " 'book',\n",
       " 'rule_book',\n",
       " 'Koran',\n",
       " 'Quran',\n",
       " \"al-Qur'an\",\n",
       " 'Book',\n",
       " 'Bible',\n",
       " 'Christian_Bible',\n",
       " 'Book',\n",
       " 'Good_Book',\n",
       " 'Holy_Scripture',\n",
       " 'Holy_Writ',\n",
       " 'Scripture',\n",
       " 'Word_of_God',\n",
       " 'Word',\n",
       " 'book',\n",
       " 'book',\n",
       " 'book',\n",
       " 'reserve',\n",
       " 'hold',\n",
       " 'book',\n",
       " 'book',\n",
       " 'book']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moral excellence or admirableness\n",
      "evil\n",
      "the quality of being morally wrong in principle or practice\n"
     ]
    }
   ],
   "source": [
    "gn2 = wordnet.synset('good.n.02')\n",
    "print(gn2.definition())\n",
    "evil = gn2.lemmas()[0].antonyms()[0]\n",
    "print(evil.name())\n",
    "print(evil.synset().definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "having desirable or positive qualities especially those suitable for a thing specified\n",
      "bad\n",
      "having undesirable or negative qualities\n"
     ]
    }
   ],
   "source": [
    "ga1 = wordnet.synset('good.a.01')\n",
    "print(ga1.definition())\n",
    "bad = ga1.lemmas()[0].antonyms()[0]\n",
    "print(bad.name())\n",
    "print(bad.synset().definition())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating WordNet Synset similarity\n",
    "\n",
    "- Using wup_similarity method to calculate the similarity for nouns\n",
    " - Wu-Palmer Similarity is a scoring method based on how similar the word senses are and where the Synsets occur relative to each other in the hypernym tree.\n",
    "- Can also use wup_similarity method to calculate the similarity for verbs\n",
    "- Two other similarity comparisons:\n",
    " - Path similarity\n",
    " - Leacock Chordorow (LCH) similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9166666666666666"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "cb = wordnet.synset('cookbook.n.01')\n",
    "ib = wordnet.synset('instruction_book.n.01')\n",
    "cb.wup_similarity(ib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38095238095238093"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dog = wordnet.synset('dog.n.01')\n",
    "dog.wup_similarity(cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cook = wordnet.synset('cook.v.01')\n",
    "bake = wordnet.synset('bake.v.02')\n",
    "cook.wup_similarity(bake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3333333333333333\n",
      "0.07142857142857142\n",
      "2.538973871058276\n",
      "0.9985288301111273\n"
     ]
    }
   ],
   "source": [
    "print(cb.path_similarity(ib))\n",
    "print(cb.path_similarity(dog))\n",
    "print(cb.lch_similarity(ib))\n",
    "print(cb.lch_similarity(dog))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discovering word collocations\n",
    "\n",
    "Collocations are two or more words that tend to appear frequently together, such as United States.\n",
    "\n",
    "- scoring functions\n",
    " - likelihood_ratio()\n",
    " - raw_freq() (check NgramAssocMeasures in the nltk.metrics)\n",
    "- scoring ngrams\n",
    " - nbest(score_fun, number)\n",
    " - above_score(score_fun, min_score): return all ngrams with scores that are at least min_score\n",
    " - score_ngrams(score_fn): return a list with tuple pairs of (ngram, score)\n",
    "### How it works\n",
    "\n",
    "BigramCollocationFinder constructs two frequency distributions: one for each word, and another for bigrams. A frequency distribution, or FreqDist in NLTK, is basically an enhanced Python dictionary where the keys are what's being counted, and the values are the counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(\"'\", 's'), ('arthur', ':'), ('#', '1'), (\"'\", 't')]\n",
      "[('black', 'knight'), ('clop', 'clop'), ('head', 'knight'), ('mumble', 'mumble'), ('squeak', 'squeak'), ('saw', 'saw'), ('holy', 'grail'), ('run', 'away'), ('french', 'guard'), ('cartoon', 'character')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import webtext\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "\n",
    "# naive approach\n",
    "words = [w.lower() for w in webtext.words('grail.txt')]\n",
    "bcf = BigramCollocationFinder.from_words(words)\n",
    "print(bcf.nbest(BigramAssocMeasures.likelihood_ratio, 4))\n",
    "\n",
    "# refine it by adding a word filter to remove punctuation and stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stopset = set(stopwords.words('english'))\n",
    "filter_stops = lambda w: len(w) < 3 or w in stopset\n",
    "bcf.apply_word_filter(filter_stops)\n",
    "print(bcf.nbest(BigramAssocMeasures.likelihood_ratio, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
